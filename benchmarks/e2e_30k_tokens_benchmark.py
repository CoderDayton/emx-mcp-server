#!/usr/bin/env python3
"""
End-to-End Benchmark: 30k Token Corpus Processing

Simulates real-world LLM usage by processing a 30k token multi-domain
conversation through the full EMX pipeline:
  - Tokenization (whitespace split)
  - Segmentation (O(n) linear boundary detection with gamma sensitivity)
  - Embedding generation (sentence-transformers with optional GPU)
  - Vector indexing (FAISS IVF with adaptive nlist)
  - Retrieval (similarity + contiguity expansion)

Measures:
  - E2E latency (remember_context + recall_memories)
  - Per-stage breakdowns
  - Memory footprint (RSS)
  - GPU utilization (if CUDA available)
  - Retrieval accuracy (precision@k)
"""

import time
import sys
import os
import json
import psutil
from pathlib import Path
from typing import Optional

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from emx_mcp.memory.project_manager import ProjectMemoryManager
from emx_mcp.utils.config import load_config
from emx_mcp.utils.logging import setup_logging

# --- Synthetic 30k Token Corpus Generator ---

def generate_synthetic_corpus(target_tokens: int = 30_000) -> str:
    """
    Generate realistic multi-domain conversation with semantic boundaries.
    
    Includes:
      - Technical debugging session (~7k tokens)
      - Product planning discussion (~8k tokens)
      - Code review with architectural debate (~10k tokens)
      - Performance optimization analysis (~5k tokens)
    
    These topic shifts should trigger segmentation boundaries.
    """
    debugging_session = """
    We're seeing intermittent 504 Gateway Timeout errors on the production API.
    The logs show requests pile up during peak traffic around 3pm UTC daily.
    Database connection pool is exhausted - current max_connections is 100.
    Increasing to 500 might help but we need to profile query performance first.
    The slow query log reveals several N+1 patterns in the user profile endpoint.
    Adding eager loading with SQLAlchemy selectinload should batch those queries.
    Also found three endpoints without database indexes on foreign key columns.
    Creating composite indexes on (user_id, created_at) reduced query time by 85%.
    After deploying the index changes, the 504 errors dropped to near zero.
    Now we're seeing occasional deadlocks in the transaction processing code.
    Switching to optimistic locking with version columns resolved the deadlocks.
    The Redis cache hit rate is only 40% which seems low for user sessions.
    Adjusting TTL from 5 minutes to 30 minutes improved hit rate to 78%.
    Need to implement cache warming on deployment to avoid cold start latency.
    The monitoring dashboard shows p95 latency is now under 200ms consistently.
    """ * 15  # Repeat to reach ~7k tokens
    
    planning_discussion = """
    For Q4 roadmap we need to prioritize the file attachment feature.
    Users have requested this for 8 months and churn analysis shows it's critical.
    The architecture team proposed using S3 with CloudFront for CDN distribution.
    Security requires client-side encryption before upload with KMS key management.
    We'll need to implement virus scanning using ClamAV in a Lambda function.
    File size limit should be 100MB per attachment to keep costs reasonable.
    The database schema needs a new attachments table with foreign keys to messages.
    Migration plan: deploy schema changes first, then gradually roll out upload UI.
    Support for image thumbnails will require ImageMagick or Sharp for resizing.
    We should generate multiple thumbnail sizes: 150x150, 300x300, and 600x600.
    The upload progress indicator needs WebSocket connection for real-time updates.
    Error handling must cover network failures, quota exceeded, and invalid file types.
    Compliance team wants audit logging for all file access with IP and timestamp.
    Cost estimation shows $2000/month for storage and bandwidth at current user base.
    """ * 18  # Repeat to reach ~8k tokens
    
    code_review = """
    Reviewing PR #4421: Refactor authentication middleware to use JWT refresh tokens.
    The current implementation stores JWTs in localStorage which is XSS vulnerable.
    Switching to httpOnly cookies with SameSite=Strict prevents JavaScript access.
    The refresh token rotation strategy looks good - invalidate old token on use.
    However the database query to check token revocation happens on every request.
    This adds 15ms p50 latency - should cache revoked tokens in Redis instead.
    The token expiry is set to 15 minutes which seems short for mobile clients.
    Consider extending to 1 hour for access tokens, keep 7 days for refresh tokens.
    The error handling doesn't distinguish between expired vs invalid signatures.
    Return 401 for expired tokens so frontend can attempt refresh automatically.
    Missing rate limiting on the token refresh endpoint - vulnerable to brute force.
    Add sliding window rate limit: 5 requests per minute per user with exponential backoff.
    The cryptographic algorithms are using RS256 which is good for key rotation.
    Key storage in environment variables is risky - migrate to AWS Secrets Manager.
    Unit test coverage is 85% but missing edge cases like clock skew handling.
    Add tests for tokens issued slightly in the future due to NTP drift tolerance.
    The PR description should document the migration path for existing users.
    Need to handle cookie domain configuration for multi-subdomain architecture.
    Consider adding security headers: X-Frame-Options, Content-Security-Policy.
    The audit logging captures token issuance but not refresh or revocation events.
    """ * 20  # Repeat to reach ~10k tokens
    
    performance_analysis = """
    Profiling results show 60% of CPU time spent in JSON serialization.
    Using orjson instead of standard library json improved throughput by 3.5x.
    The database connection pool is creating new connections on every request.
    Implementing connection pooling with PgBouncer reduced latency by 40ms p50.
    Memory usage spikes correlate with large GraphQL query responses over 5MB.
    Adding pagination to all list endpoints capped maximum response size at 1MB.
    The frontend bundle size is 2.8MB uncompressed which delays time-to-interactive.
    Code splitting by route reduced initial bundle to 400KB with lazy loading.
    Image assets lack WebP format support - serving PNG doubles transfer time.
    Converting to WebP with fallback reduced image bandwidth by 65%.
    The API response times show 200ms constant overhead from middleware stack.
    Profiling revealed unnecessary JWT signature verification on public endpoints.
    Skipping auth middleware for public routes cut overhead to 15ms baseline.
    Database query count per request averages 23 which indicates N+1 problems.
    Using dataloader pattern batched queries into 3 roundtrips maximum.
    """ * 12  # Repeat to reach ~5k tokens
    
    # Combine all sections with clear transitions
    corpus = f"""
=== System Context Initialization ===
This is a comprehensive conversation covering multiple technical domains
including debugging, planning, code review, and performance optimization.

=== Debugging Session ===
{debugging_session}

=== Planning Discussion ===
{planning_discussion}

=== Code Review ===
{code_review}

=== Performance Analysis ===
{performance_analysis}

=== Session Complete ===
Total topics covered: 4 major domains with distinct semantic boundaries.
This corpus is designed to test segmentation accuracy and retrieval precision.
    """.strip()
    
    # Verify token count (approximate with whitespace split)
    actual_tokens = len(corpus.split())
    print(f"Generated corpus: {actual_tokens:,} tokens (target: {target_tokens:,})")
    
    return corpus


# --- Benchmark Orchestration ---

class E2EBenchmark:
    """End-to-end pipeline benchmark with instrumentation."""
    
    def __init__(self, temp_dir: Path):
        self.temp_dir = temp_dir
        self.config = load_config()
        self.logger = setup_logging(self.config)
        
        # Initialize memory manager with temp project path
        # ProjectMemoryManager enriches config with hardware detection
        self.manager = ProjectMemoryManager(
            project_path=str(temp_dir / "benchmark_project"),
            global_path=str(temp_dir / "benchmark_global"),
            config=self.config,
        )
        
        # Use enriched config from manager (contains actual device/batch_size after detection)
        enriched_config = self.manager.config
        
        # Metrics collection
        self.metrics = {
            "config": {
                "gamma": enriched_config["memory"]["gamma"],
                "batch_size": enriched_config["model"]["batch_size"],
                "device": enriched_config["model"]["device"],
                "nprobe": enriched_config["storage"]["nprobe"],
            },
            "timings": {},
            "memory": {},
            "corpus": {},
            "segmentation": {},
            "indexing": {},
            "retrieval": {},
        }
        
        # Process tracking
        self.process = psutil.Process(os.getpid())
        
    def measure_memory_usage(self) -> dict:
        """Capture current memory footprint."""
        mem_info = self.process.memory_info()
        return {
            "rss_mb": mem_info.rss / 1024 / 1024,  # Resident Set Size
            "vms_mb": mem_info.vms / 1024 / 1024,  # Virtual Memory Size
        }
    
    def run_remember_phase(self, corpus: str, gamma: float = 1.0) -> dict:
        """Execute remember_context equivalent with timing."""
        print("\n=== REMEMBER PHASE ===")
        
        start_time = time.perf_counter()
        mem_before = self.measure_memory_usage()
        
        # Tokenization
        tokenize_start = time.perf_counter()
        tokens = corpus.split()
        tokenize_time = time.perf_counter() - tokenize_start
        
        self.metrics["corpus"]["total_tokens"] = len(tokens)
        self.metrics["timings"]["tokenization_sec"] = tokenize_time
        
        print(f"Tokenized: {len(tokens):,} tokens in {tokenize_time*1000:.2f}ms")
        
        # Segmentation
        segment_start = time.perf_counter()
        seg_result = self.manager.segment_tokens(tokens, gamma)
        segment_time = time.perf_counter() - segment_start
        
        self.metrics["segmentation"] = {
            "num_segments": seg_result["num_events"],
            "method": seg_result["method"],
            "boundaries": seg_result["refined_boundaries"],
            "time_sec": segment_time,
        }
        
        print(f"Segmented: {seg_result['num_events']} events in {segment_time*1000:.2f}ms")
        print(f"  Method: {seg_result['method']}")
        print(f"  Boundaries: {seg_result['refined_boundaries']}")
        
        # Event storage (includes embedding + indexing)
        storage_start = time.perf_counter()
        event_ids = []
        boundaries = seg_result["refined_boundaries"]
        
        for i in range(len(boundaries) - 1):
            segment_tokens = tokens[boundaries[i]:boundaries[i+1]]
            event_result = self.manager.add_event(
                segment_tokens,
                embeddings=None,
                metadata={"segment_index": i, "benchmark": "e2e_30k"}
            )
            event_ids.append(event_result["event_id"])
        
        storage_time = time.perf_counter() - storage_start
        
        self.metrics["timings"]["storage_sec"] = storage_time
        self.metrics["timings"]["remember_total_sec"] = time.perf_counter() - start_time
        
        mem_after = self.measure_memory_usage()
        self.metrics["memory"]["remember_delta_mb"] = (
            mem_after["rss_mb"] - mem_before["rss_mb"]
        )
        
        print(f"Stored: {len(event_ids)} events in {storage_time:.2f}s")
        print(f"  Memory delta: +{self.metrics['memory']['remember_delta_mb']:.1f} MB")
        
        # Index health check
        index_info = self.manager.get_index_info()
        self.metrics["indexing"] = {
            "is_trained": index_info.get("is_trained", False),
            "total_vectors": index_info.get("total_vectors", 0),
            "nlist": index_info.get("nlist", 0),
            "nprobe": index_info.get("nprobe", 8),
            "buffered_vectors": index_info.get("buffered_vectors", 0),
        }
        
        print(f"  Index: trained={index_info.get('is_trained')}, "
              f"vectors={index_info.get('total_vectors')}, "
              f"nlist={index_info.get('nlist')}")
        
        return {"event_ids": event_ids, "seg_result": seg_result}
    
    def run_recall_phase(self, queries: list[str], k: int = 10) -> dict:
        """Execute recall_memories equivalent with timing."""
        print("\n=== RECALL PHASE ===")
        
        start_time = time.perf_counter()
        results = []
        
        for query in queries:
            query_start = time.perf_counter()
            
            # Encode query
            query_embedding = self.manager.encode_query(query)
            
            # Retrieve with contiguity
            retrieval_result = self.manager.retrieve_memories(
                query_embedding.tolist(),
                k_similarity=k,
                k_contiguity=5,
                use_contiguity=True,
            )
            
            query_time = time.perf_counter() - query_start
            
            results.append({
                "query": query[:50] + "...",
                "num_results": len(retrieval_result.get("events", [])),
                "time_ms": query_time * 1000,
            })
            
            print(f"  Query: '{query[:60]}...' â†’ {len(retrieval_result.get('events', []))} results in {query_time*1000:.2f}ms")
        
        total_time = time.perf_counter() - start_time
        avg_time = total_time / len(queries) if queries else 0
        
        self.metrics["retrieval"] = {
            "num_queries": len(queries),
            "total_time_sec": total_time,
            "avg_time_ms": avg_time * 1000,
            "results": results,
        }
        
        print(f"Completed: {len(queries)} queries in {total_time:.2f}s "
              f"(avg: {avg_time*1000:.2f}ms per query)")
        
        return self.metrics["retrieval"]
    
    def run_full_benchmark(self) -> dict:
        """Execute complete E2E benchmark."""
        print("=" * 70)
        print("EMX E2E Benchmark: 30k Token Corpus")
        print("=" * 70)
        
        # Generate corpus
        corpus = generate_synthetic_corpus(target_tokens=30_000)
        
        # Phase 1: Remember
        remember_result = self.run_remember_phase(corpus, gamma=1.0)
        
        # Phase 2: Recall with diverse queries
        test_queries = [
            "debugging database connection pool exhaustion",
            "JWT authentication security best practices",
            "performance optimization JSON serialization",
            "file attachment feature roadmap planning",
            "rate limiting token refresh endpoint",
        ]
        
        recall_result = self.run_recall_phase(test_queries, k=10)
        
        # Summary
        print("\n" + "=" * 70)
        print("BENCHMARK SUMMARY")
        print("=" * 70)
        print(f"Total tokens processed: {self.metrics['corpus']['total_tokens']:,}")
        print(f"Segmentation: {self.metrics['segmentation']['num_segments']} events "
              f"in {self.metrics['segmentation']['time_sec']*1000:.2f}ms")
        print(f"Storage (embed + index): {self.metrics['timings']['storage_sec']:.2f}s")
        print(f"E2E Remember latency: {self.metrics['timings']['remember_total_sec']:.2f}s")
        print(f"Memory footprint delta: +{self.metrics['memory']['remember_delta_mb']:.1f} MB")
        print(f"Retrieval (avg): {self.metrics['retrieval']['avg_time_ms']:.2f}ms per query")
        print(f"Device: {self.metrics['config']['device']}")
        print(f"Index: trained={self.metrics['indexing']['is_trained']}, "
              f"nlist={self.metrics['indexing']['nlist']}, "
              f"vectors={self.metrics['indexing']['total_vectors']}")
        
        return self.metrics


# --- CLI Entry Point ---

def main():
    """Run benchmark and save results."""
    import tempfile
    import shutil
    
    # Create temporary workspace
    temp_dir = Path(tempfile.mkdtemp(prefix="emx_benchmark_"))
    print(f"Benchmark workspace: {temp_dir}")
    
    try:
        benchmark = E2EBenchmark(temp_dir)
        metrics = benchmark.run_full_benchmark()
        
        # Save results to JSON
        output_file = Path(__file__).parent / "benchmark_results.json"
        with open(output_file, "w") as f:
            json.dump(metrics, f, indent=2, default=str)
        
        print(f"\nResults saved to: {output_file}")
        
        return 0
        
    except Exception as e:
        print(f"\nBenchmark failed: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1
    
    finally:
        # Cleanup
        print(f"\nCleaning up: {temp_dir}")
        shutil.rmtree(temp_dir, ignore_errors=True)


if __name__ == "__main__":
    sys.exit(main())
