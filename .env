# ============================================================
# EMX-MCP Server Configuration
# ============================================================
# Speed-optimized defaults for high-performance operation.
# Designed for GPU acceleration (CUDA) with CPU fallback.
#
# To override: Add environment variables to your MCP client
# configuration under "env": {} - those values override these.
#
# üìñ See docs/ENVIRONMENT_VARIABLES.md for detailed documentation
# ============================================================

# ============================================================
# Model Configuration
# ============================================================

# Embedding model for semantic encoding
# Default: all-MiniLM-L6-v2 (384-dim, fastest, excellent quality)
# Alternative: all-mpnet-base-v2 (768-dim, better but 3x slower)
EMX_MODEL_NAME=all-MiniLM-L6-v2

# Hardware device for inference
# Options: cpu | cuda
# Default: auto-detect (use CUDA if available, fallback to CPU)
# Speed: Explicitly set to "cuda" for GPU systems to skip detection
# EMX_MODEL_DEVICE=cuda

# Inference batch size
# Default: auto-scale (GPU: 64-512 based on VRAM, CPU: 64)
# Speed: Set to 128-256 for GPU to maximize throughput
# Range: 1-512
# EMX_MODEL_BATCH_SIZE=256

# ============================================================
# Memory Configuration - Segmentation & Context
# ============================================================

# Boundary detection sensitivity (threshold = mean + gamma √ó stddev)
# Default: 1.0 (balanced)
# Speed: Lower = more boundaries = slower retrieval
# Range: 0.1-10.0
EMX_MEMORY_GAMMA=1.0

# Context window for contextual embedding (tokens)
# Default: 10 (fast, sufficient for most text)
# Speed: Lower = faster encoding, less semantic context
# Range: 1-100
EMX_MEMORY_CONTEXT_WINDOW=10

# Lookback window for adaptive threshold calculation
# Default: 128 (fast, responsive to topic shifts)
# Speed: Directly impacts embedding computation cost
# Range: 1-2048
EMX_MEMORY_WINDOW_OFFSET=128

# Minimum tokens per episode (prevents micro-segmentation)
# Default: 8 (fast, ~1-2 sentences)
# Speed: Higher = fewer episodes = less storage overhead
# Range: 1-1024
EMX_MEMORY_MIN_BLOCK_SIZE=8

# Maximum tokens per episode (forces splits for long segments)
# Default: 128 (fast, ~1-2 paragraphs)
# Speed: Lower = more frequent indexing but better retrieval precision
# Range: 1-4096
EMX_MEMORY_MAX_BLOCK_SIZE=128

# ============================================================
# Memory Configuration - Three-Tier System
# ============================================================

# Tier 1: Initial tokens always kept (attention sinks, system prompts)
# Default: 128
# Speed: Minimal impact on performance
# Range: 1-10000
EMX_MEMORY_N_INIT=128

# Tier 2: Recent tokens in rolling window (working memory)
# Default: 4096 (fast, covers typical multi-turn conversations)
# Speed: Higher = more memory but no retrieval cost
# Range: 1-100000
EMX_MEMORY_N_LOCAL=4096

# Tier 3: Retrieved episodic memories (FAISS-indexed)
# Default: 2048 (fast, balances retrieval quality and speed)
# Speed: Higher = more vectors to search = slower retrieval
# Range: 1-50000
EMX_MEMORY_N_MEM=2048

# Representative tokens per episode for indexing
# Default: 4 (fast, optimal for most use cases)
# Speed: Higher = better episode representation but more storage
# Range: 1-100
EMX_MEMORY_REPR_TOPK=4

# Graph-based boundary refinement metric
# Options: modularity | conductance | coverage
# Default: modularity (best all-around, fast)
# Speed: All options have similar performance
EMX_MEMORY_REFINEMENT_METRIC=modularity

# ============================================================
# Storage Configuration - Vector Index
# ============================================================

# Embedding vector dimension (auto-detected from model if unset)
# Default: auto-detect from EMX_MODEL_NAME (recommended)
# all-MiniLM-L6-v2: 384 | all-mpnet-base-v2: 768
# Speed: Lower dimensions = faster search and less memory
# ‚ö†Ô∏è  ONLY set manually if you need to override auto-detection
# Range: 1-4096
# EMX_STORAGE_VECTOR_DIM=384

# FAISS IVF clusters to search (higher = better recall, slower)
# Default: 8 (fast, low-latency search)
# Speed: 4-8 for speed-critical, 16-32 for quality-critical
# Range: 1-1024
EMX_STORAGE_NPROBE=8

# Vectors before switching from memory to disk-backed index
# Default: 300000 (fast, ~400MB for 384-dim)
# Speed: Higher = faster retrieval but more RAM usage
# Range: 1000-10000000
EMX_STORAGE_DISK_OFFLOAD_THRESHOLD=300000

# Minimum vectors before training IVF index (uses Flat below)
# Default: 1000 (fast, optimal for IVF training stability)
# Speed: Don't change unless you understand FAISS internals
# Range: 10-1000000
EMX_STORAGE_MIN_TRAINING_SIZE=1000

# FAISS index type
# Options: IVF | Flat | HNSW
# Default: IVF (fast approximate search for >10k vectors)
# Speed: IVF is fastest for large-scale, Flat for <5k exact search
EMX_STORAGE_INDEX_TYPE=IVF

# Distance metric for vector similarity
# Options: cosine | euclidean | dot
# Default: cosine (best for semantic similarity)
# Speed: dot is fastest but less intuitive for semantics
EMX_STORAGE_METRIC=cosine

# Automatic IVF index retraining when nlist drift detected
# Default: true (keeps index near-optimal as vectors grow)
# Speed: true = better long-term performance, minimal overhead
EMX_STORAGE_AUTO_RETRAIN=true

# Ratio threshold for triggering automatic index retraining
# Default: 2.0 (balanced, retrains when drift >2x optimal)
# Speed: Lower = more retraining = better search quality
# Range: 1.1-10.0
EMX_STORAGE_NLIST_DRIFT_THRESHOLD=2.0

# ============================================================
# GPU Optimization Configuration
# ============================================================

# Enable pinned memory pool for async CPU‚ÜíGPU transfers
# Default: true (non-blocking transfers, requires PyTorch+CUDA)
# Speed: true = 10-20% faster GPU encoding for large batches
# Only beneficial for batch_size ‚â• 32
EMX_GPU_ENABLE_PINNED_MEMORY=true

# Number of reusable pinned memory buffers in pool
# Default: 4 (sufficient for low-medium concurrency)
# Speed: 8-16 for high concurrency, 2-4 for single-threaded
# Range: 1-32
EMX_GPU_PINNED_BUFFER_SIZE=4

# Maximum batch size per pinned buffer
# Default: 128 (matches typical GPU batch sizes)
# Speed: Should match or exceed EMX_MODEL_BATCH_SIZE
# Range: 32-512
EMX_GPU_PINNED_MAX_BATCH=256

# Minimum batch size to use pinned memory (overhead below this)
# Default: 64 (crossover point where async benefits exceed overhead)
# Speed: Lower if fast DMA, higher if slow CPU-GPU link
# Range: 1-256
EMX_GPU_PINNED_MIN_BATCH_THRESHOLD=64

# ============================================================
# Metrics Configuration - OpenTelemetry
# ============================================================

# Service name reported in metrics/traces
# Default: emx-mcp-server
# Keep consistent across environments for tracking
OTEL_SERVICE_NAME=emx-mcp-server

# Environment label for metrics filtering
# Options: development | staging | production
# Default: development
OTEL_ENVIRONMENT=development

# OTLP metrics endpoint for remote observability backends
# Default: disabled (console export only)
# Examples:
#   Grafana Cloud: https://otlp-gateway-<region>.grafana.net/otlp/v1/metrics
#   Honeycomb: https://api.honeycomb.io:443/v1/metrics
#   Local Collector: http://localhost:4318/v1/metrics
# Speed: No performance impact, async background export
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318/v1/metrics

# OTLP authentication headers (for Grafana Cloud, Honeycomb, etc.)
# Format: key1=value1,key2=value2
# Example (Grafana Cloud): Authorization=Basic <base64-encoded-credentials>
# Speed: No performance impact
# OTEL_EXPORTER_OTLP_HEADERS=Authorization=Basic <your-credentials>

# Interval between metric exports (milliseconds)
# Default: 10000 (10 seconds, balanced)
# Speed: Higher = less backend load but delayed visibility
# Range: 1000-300000 (1s-5min)
OTEL_METRIC_EXPORT_INTERVAL=10000

# Enable console metrics exporter (writes to stderr)
# Default: false (production, use OTLP instead)
# Speed: Minimal impact, useful for debugging only
# ‚ö†Ô∏è  MCP servers use STDIO (stdout = protocol), console goes to stderr
OTEL_METRICS_CONSOLE=false

# ============================================================
# Logging Configuration
# ============================================================

# Minimum log level
# Options: DEBUG | INFO | WARNING | ERROR | CRITICAL
# Default: INFO (production, shows major operations)
# Speed: WARNING/ERROR for minimal overhead
EMX_LOGGING_LEVEL=INFO

# Log message format
# Default: timestamp + name + level + message
EMX_LOGGING_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s

# ============================================================
# Runtime Paths
# ============================================================

# Project path override (default: auto-detect from git root or cwd)
# Use to force specific project when running from subdirectory
# Speed: No impact
# EMX_PROJECT_PATH=/path/to/your/project

# Global memory storage location (default: ~/.emx-mcp/global_memories)
# Shared across all projects for cross-project knowledge
# Speed: No impact, ensure path is on fast storage (SSD)
# EMX_GLOBAL_PATH=/path/to/global/memories

# ============================================================
# Speed Optimization Summary
# ============================================================
# For maximum throughput with GPU:
#   EMX_MODEL_DEVICE=cuda
#   EMX_MODEL_BATCH_SIZE=256
#   EMX_GPU_ENABLE_PINNED_MEMORY=true
#   EMX_GPU_PINNED_BUFFER_SIZE=8
#   EMX_GPU_PINNED_MAX_BATCH=256
#   EMX_STORAGE_NPROBE=8
#   EMX_LOGGING_LEVEL=WARNING
#
# For CPU-only systems:
#   EMX_MODEL_DEVICE=cpu
#   EMX_MODEL_BATCH_SIZE=64
#   EMX_GPU_ENABLE_PINNED_MEMORY=false
#   EMX_STORAGE_NPROBE=4
# ============================================================
